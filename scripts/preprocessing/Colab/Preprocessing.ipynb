{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "IMPORTANT:\n",
        "This project is designed to run exclusively on Google Colab.\n",
        "\n",
        "It relies on Google Drive being mounted at:\n",
        "    /content/drive/MyDrive/\n",
        "\n",
        "Local execution is not supported.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "s5k1ZB5BYmFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOUNTS DRIVE**"
      ],
      "metadata": {
        "id": "_NBn8bQZEvgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mounts Google Drive into the Colab environment to access project files\n",
        "\n",
        "# Changes the current working directory to the NeuroScape project folder in Google Drive\n",
        "%cd /content/drive/MyDrive/NeuroScape"
      ],
      "metadata": {
        "id": "mTAJr3YuExwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d513e54-4692-4a0b-c239-6d0f47f4f64e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1vM2NZYPQBx0CCXgmkPKl1lguMSLh85MO/NeuroScape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "t0sDacauG7FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                      # Operating system interactions (paths, directories, environment variables)\n",
        "import glob                    # File pattern matching to find datasets or shards\n",
        "import time                    # Timing and sleep operations\n",
        "import pandas as pd            # Data manipulation and analysis (DataFrames)\n",
        "import torch                   # Core PyTorch library for deep learning\n",
        "import numpy as np             # Numerical computing (arrays, matrix operations)\n",
        "import torch.nn as nn          # PyTorch module for building neural networks\n",
        "import sys                     # System-specific parameters and functions (e.g., path manipulation)\n",
        "import pickle                  # Save/load Python objects (like trained models or embeddings)\n",
        "import tomllib                 # Parse TOML configuration files\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # Split dataset into train/test sets\n",
        "\n",
        "# Project-specific utilities for the discipline classifier\n",
        "from src.utils.parsing import parse_directories\n",
        "from src.utils.classifier import (\n",
        "    load_configurations,         # Load hyperparameters and settings from config\n",
        "    save_and_create_dataset,     # Preprocess data and save as structured dataset\n",
        "    load_shard,                  # Load a shard of the dataset (for large data)\n",
        "    extract_data,                # Extract features and labels from shards\n",
        "    get_disciplines,             # List the disciplines present in the dataset\n",
        "    generate_n_hot_vector,       # Convert labels to multi-hot vectors\n",
        "    get_unique_disciplines_and_count, # Get all unique disciplines and their counts\n",
        "    train_one_epoch,             # Train the model for one epoch\n",
        "    validate,                    # Validate model performance on test set\n",
        "    save_model,                  # Save trained model to disk\n",
        "    data_loader,                 # Create PyTorch DataLoader from dataset\n",
        "    to_device,                   # Move tensors or models to GPU/CPU\n",
        "    compute_expected_accuracy,   # Compute accuracy metric\n",
        "    compute_kappa,               # Compute Cohen's Kappa metric\n",
        "    drop_class                   # Remove classes with insufficient examples\n",
        ")\n",
        "\n",
        "# Data type definitions\n",
        "from src.classes.data_types import EmbeddingsWithLabels  # Holds embeddings along with corresponding labels\n",
        "from src.classes.discipline_classifier import DisciplineClassifier  # Neural network model for classifying disciplines\n",
        "\n",
        "# Environment management\n",
        "from dotenv import load_dotenv, find_dotenv  # Load environment variables for sensitive keys or paths\n",
        "\n",
        "from copy import deepcopy  # Deep copy objects to prevent mutation\n"
      ],
      "metadata": {
        "id": "nWb_gTSbFyeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Classifier Training Data**"
      ],
      "metadata": {
        "id": "LJ9oPivqEf6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61B5NgdIA71U"
      },
      "outputs": [],
      "source": [
        "from src.utils.parsing import parse_directories\n",
        "from src.classes.data_types import EmbeddingsWithLabels\n",
        "from src.utils.classifier import load_configurations, save_and_create_dataset, \\\n",
        "    load_shard, extract_data, get_disciplines, generate_n_hot_vector, get_unique_disciplines_and_count\n",
        "\n",
        "# Load environment variables from the keys.env file\n",
        "load_dotenv(\"/content/drive/MyDrive/NeuroScape/keys.env\")\n",
        "BASEPATH = os.environ['BASEPATH']\n",
        "\n",
        "seed = int(time.time())  # Random seed based on current time\n",
        "\n",
        "def process_shards(shards,\n",
        "                   dataframe,\n",
        "                   unique_disciplines,\n",
        "                   shard_ids,\n",
        "                   directories,\n",
        "                   set_type,\n",
        "                   threshold,\n",
        "                   delete_shards=False):\n",
        "    \"\"\"\n",
        "    Process a list of shards and split the data into single-label (monolabel)\n",
        "    and multi-label (multilabel) datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - shards (list): List of embedding shard files to process.\n",
        "    - dataframe (pd.DataFrame): DataFrame containing articles and their disciplines.\n",
        "    - unique_disciplines (list): All unique disciplines in the dataset.\n",
        "    - shard_ids (tuple): Current IDs for the mono and multi datasets.\n",
        "    - directories (tuple): Directories to save monolabel and multilabel datasets.\n",
        "    - set_type (str): Type of dataset ('Train', 'Val', 'Test').\n",
        "    - threshold (int): Max number of items per dataset file before saving.\n",
        "    - delete_shards (bool): Whether to delete shard files after processing.\n",
        "\n",
        "    Returns:\n",
        "    - id_mono (int): Updated ID for monolabel dataset.\n",
        "    - id_multi (int): Updated ID for multilabel dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize empty containers for mono (single-label) and multi (multi-label) data\n",
        "    mono_data = EmbeddingsWithLabels(pmids=[], embeddings=[], labels=[])\n",
        "    multi_data = EmbeddingsWithLabels(pmids=[], embeddings=[], labels=[])\n",
        "    num_mono = 0\n",
        "    num_multi = 0\n",
        "\n",
        "    id_mono, id_multi = shard_ids\n",
        "    monolabel_directory, multilabel_directory = directories\n",
        "\n",
        "    for shard in shards:\n",
        "        # Save datasets if threshold is reached\n",
        "        if num_mono >= threshold:\n",
        "            mono_data, id_mono = save_and_create_dataset(\n",
        "                mono_data, id_mono, monolabel_directory, set_type)\n",
        "            num_mono = 0\n",
        "\n",
        "        if num_multi >= threshold:\n",
        "            multi_data, id_multi = save_and_create_dataset(\n",
        "                multi_data, id_multi, multilabel_directory, set_type)\n",
        "            num_multi = 0\n",
        "\n",
        "        # Load embeddings from the shard\n",
        "        abstracts = load_shard(shard)\n",
        "        if delete_shards:\n",
        "            os.remove(shard)  # Optionally delete the shard after processing\n",
        "\n",
        "        # Retrieve disciplines for each article in the shard\n",
        "        shard_disciplines = [\n",
        "            get_disciplines(dataframe, pmid) for pmid in abstracts.pmids\n",
        "        ]\n",
        "\n",
        "        # Convert disciplines into n-hot vectors for multilabel classification\n",
        "        n_hot_vector, num_hot = generate_n_hot_vector(shard_disciplines, unique_disciplines)\n",
        "\n",
        "        # ------------------------\n",
        "        # MONOLABEL: articles with exactly one discipline\n",
        "        # ------------------------\n",
        "        mono_pmids, mono_embeddings, mono_labels = extract_data(\n",
        "            abstracts, n_hot_vector, num_hot, lambda x: x == 1)\n",
        "        mono_data.pmids.extend(mono_pmids)\n",
        "        mono_data.embeddings.extend(mono_embeddings)\n",
        "        mono_data.labels.extend(mono_labels)\n",
        "        num_mono += len(mono_pmids)\n",
        "\n",
        "        # ------------------------\n",
        "        # MULTILABEL: articles with more than one discipline\n",
        "        # ------------------------\n",
        "        multi_pmids, multi_embeddings, multi_labels = extract_data(\n",
        "            abstracts, n_hot_vector, num_hot, lambda x: x > 1)\n",
        "        multi_data.pmids.extend(multi_pmids)\n",
        "        multi_data.embeddings.extend(multi_embeddings)\n",
        "        multi_data.labels.extend(multi_labels)\n",
        "        num_multi += len(multi_pmids)\n",
        "\n",
        "    # Save any remaining articles after processing all shards\n",
        "    if len(mono_data.pmids) > 0:\n",
        "        mono_data, id_mono = save_and_create_dataset(mono_data, id_mono, monolabel_directory, set_type)\n",
        "\n",
        "    if len(multi_data.pmids) > 0:\n",
        "        multi_data, id_multi = save_and_create_dataset(multi_data, id_multi, multilabel_directory, set_type)\n",
        "\n",
        "    return id_mono, id_multi\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    configurations = load_configurations()  # Load hyperparameters and other settings\n",
        "\n",
        "    data_directories = parse_directories()  # Retrieve project directory paths\n",
        "\n",
        "    # Paths for cleaned CSV data and embeddings\n",
        "    dataframe_directory = os.path.join('/content/drive/MyDrive/NeuroScape/output/tratados/')\n",
        "    embedding_directory = os.path.join('/content/drive/MyDrive/NeuroScape/output/embeddings')\n",
        "\n",
        "    other_dataframe_directory = os.path.join(dataframe_directory, 'otherdisciplines')\n",
        "    neuro_dataframe_directory = os.path.join(dataframe_directory, 'neuroscience')\n",
        "\n",
        "    other_embedding_directory = os.path.join(embedding_directory, 'otherdisciplines')\n",
        "    neuro_embedding_directory = os.path.join(embedding_directory, 'neuroscience')\n",
        "\n",
        "    # Directories to store monolabel and multilabel datasets\n",
        "    multilabel_directory = os.path.join(\n",
        "        BASEPATH, data_directories['internal']['intermediate']['classifier'], 'Multilabel')\n",
        "    monolabel_directory = os.path.join(\n",
        "        BASEPATH, data_directories['internal']['intermediate']['classifier'], 'Monolabel')\n",
        "\n",
        "    # Ensure all directories exist for Train, Validation, and Test splits\n",
        "    os.makedirs(os.path.join(multilabel_directory, 'Train'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(multilabel_directory, 'Val'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(multilabel_directory, 'Test'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(monolabel_directory, 'Train'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(monolabel_directory, 'Val'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(monolabel_directory, 'Test'), exist_ok=True)\n",
        "\n",
        "    # Load CSVs\n",
        "    other_dataframe = pd.read_csv(os.path.join(other_dataframe_directory, 'articles_merged_cleaned.csv'))\n",
        "    neuro_dataframe = pd.read_csv(os.path.join(neuro_dataframe_directory, 'articles_merged_cleaned.csv'))\n",
        "\n",
        "    # Load embedding shards\n",
        "    other_shards = glob.glob(os.path.join(other_embedding_directory, '*.pkl'))\n",
        "    neuro_shards = glob.glob(os.path.join(neuro_embedding_directory, '*.pkl'))\n",
        "\n",
        "    # Get all unique disciplines and their count\n",
        "    unique_disciplines, num_classes = get_unique_disciplines_and_count(other_dataframe)\n",
        "\n",
        "    directories = (monolabel_directory, multilabel_directory)\n",
        "    threshold = configurations['preparation']['item_threshold']\n",
        "\n",
        "    # Split \"other disciplines\" into Train, Val, and Test\n",
        "    train_val_shards, test_shards = train_test_split(other_shards, test_size=0.1, random_state=seed)\n",
        "    train_shards, val_shards = train_test_split(train_val_shards, test_size=0.1, random_state=seed)\n",
        "\n",
        "    shard_ids = (0, 0)  # Initial IDs for mono and multi datasets\n",
        "\n",
        "    print('Processing training shards of other disciplines...')\n",
        "    other_train_ids = process_shards(train_shards, other_dataframe, unique_disciplines, shard_ids,\n",
        "                                     directories, 'Train', threshold)\n",
        "\n",
        "    print('Processing validation of other disciplines...')\n",
        "    other_val_ids = process_shards(val_shards, other_dataframe, unique_disciplines, shard_ids,\n",
        "                                   directories, 'Val', threshold)\n",
        "\n",
        "    print('Processing test shards of other disciplines...')\n",
        "    other_test_ids = process_shards(test_shards, other_dataframe, unique_disciplines, shard_ids,\n",
        "                                    directories, 'Test', threshold)\n",
        "\n",
        "    # Use a small portion of neuroscience shards to augment datasets\n",
        "    used_shards, _ = train_test_split(neuro_shards, test_size=0.9, random_state=seed)\n",
        "    train_val_shards, test_shards = train_test_split(used_shards, test_size=0.1, random_state=seed)\n",
        "    train_shards, val_shards = train_test_split(train_val_shards, test_size=0.1, random_state=seed)\n",
        "\n",
        "    print('Processing training shards of neuroscience...')\n",
        "    neuro_train_ids = process_shards(train_shards, neuro_dataframe, unique_disciplines, other_train_ids,\n",
        "                                     directories, 'Train', threshold, delete_shards=True)\n",
        "\n",
        "    print('Processing validation shards of neuroscience...')\n",
        "    neuro_val_ids = process_shards(val_shards, neuro_dataframe, unique_disciplines, other_val_ids,\n",
        "                                   directories, 'Val', threshold, delete_shards=True)\n",
        "\n",
        "    print('Processing test shards of neuroscience...')\n",
        "    neuro_test_ids = process_shards(test_shards, neuro_dataframe, unique_disciplines, other_test_ids,\n",
        "                                    directories, 'Test', threshold, delete_shards=True)\n",
        "\n",
        "    print('Data preparation completed.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Discipline Classifier**"
      ],
      "metadata": {
        "id": "OXUqY2SIEkZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils.parsing import parse_directories\n",
        "from src.classes.discipline_classifier import DisciplineClassifier\n",
        "from src.utils.classifier import load_configurations, train_one_epoch, validate, \\\n",
        "    save_model, data_loader, to_device, compute_expected_accuracy, compute_kappa, \\\n",
        "    drop_class\n",
        "\n",
        "# Ensure the project path is correct\n",
        "sys.path.append('/content/drive/MyDrive/NeuroScape/')\n",
        "os.chdir(\"/content/drive/MyDrive/NeuroScape\")\n",
        "print(\"Working directory:\", os.getcwd())\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(\"/content/drive/MyDrive/NeuroScape/keys.env\")\n",
        "BASEPATH = os.environ['BASEPATH']\n",
        "\n",
        "# TRAINING FUNCTION\n",
        "def train_model(model, filter_model, data_directory, model_directory,\n",
        "                model_name, configurations, device, loss_function, optimizer):\n",
        "    \"\"\"\n",
        "    Train a discipline classifier model with optional filtering on multi-label datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - model: PyTorch model to train\n",
        "    - filter_model: Optional pretrained model used to filter uncertain labels\n",
        "    - data_directory: Directory containing training/validation data (Mono or Multi)\n",
        "    - model_directory: Directory to save the trained model\n",
        "    - model_name: Name of the saved model file\n",
        "    - configurations: Training hyperparameters (epochs, batch_size, etc.)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    - loss_function: Loss function for training (BCELoss)\n",
        "    - optimizer: Optimizer (Adam)\n",
        "\n",
        "    Returns:\n",
        "    - best_model: Model with lowest validation loss\n",
        "    \"\"\"\n",
        "\n",
        "    confidence_cutoff = configurations['confidence_cutoff']\n",
        "    epochs = configurations['epochs']\n",
        "    buffer_size = configurations['buffer_size']\n",
        "    batch_size = configurations['batch_size']\n",
        "    save_path = os.path.join(model_directory, f'{model_name}.pth')\n",
        "    best_loss = float('inf')\n",
        "    best_model = None\n",
        "\n",
        "    # Load training and validation shards\n",
        "    train_files = glob.glob(os.path.join(data_directory, 'Train/*.pkl'))\n",
        "    val_files = glob.glob(os.path.join(data_directory, 'Val/*.pkl'))\n",
        "    X_val, Y_val = data_loader(val_files)\n",
        "\n",
        "    # If using a filter model, drop uncertain labels from validation set\n",
        "    if filter_model is not None:\n",
        "        Y_val = drop_class(filter_model, X_val, Y_val, device, confidence_cutoff)\n",
        "\n",
        "    expected_accuracy = compute_expected_accuracy(Y_val)\n",
        "    X_val, Y_val = to_device(X_val, Y_val, device)\n",
        "\n",
        "    print(f\"Expected Accuracy: {expected_accuracy:.4f}\")\n",
        "    print('---' * 10)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        average_loss = 0\n",
        "        total_samples = 0\n",
        "        np.random.shuffle(train_files)  # Shuffle the shards for each epoch\n",
        "\n",
        "        for i in range(0, len(train_files), buffer_size):\n",
        "            files = train_files[i:i + buffer_size]\n",
        "\n",
        "            X, Y = data_loader(files)\n",
        "\n",
        "            # Skip invalid shards\n",
        "            if X is None or len(X) == 0 or X.shape[1] == 0:\n",
        "                print(f\"[WARN] Ignoring invalid shard: {files} (X shape = {None if X is None else X.shape})\")\n",
        "                continue\n",
        "\n",
        "            # Apply filter_model to remove uncertain multi-label data if applicable\n",
        "            if filter_model is not None:\n",
        "                Y = drop_class(filter_model, X, Y, device, confidence_cutoff)\n",
        "\n",
        "            total_samples += len(X)\n",
        "\n",
        "            # Train in mini-batches\n",
        "            for j in range(0, len(X), batch_size):\n",
        "                X_batch = X[j:j + batch_size]\n",
        "                Y_batch = Y[j:j + batch_size]\n",
        "\n",
        "                X_batch, Y_batch = to_device(X_batch, Y_batch, device)\n",
        "\n",
        "                loss = train_one_epoch(model, X_batch, Y_batch, loss_function, optimizer)\n",
        "                average_loss += loss\n",
        "\n",
        "        average_loss /= total_samples\n",
        "\n",
        "        # Validation after each epoch\n",
        "        val_loss, val_accuracy = validate(model, X_val, Y_val, loss_function)\n",
        "        kappa = compute_kappa(val_accuracy, expected_accuracy)\n",
        "\n",
        "        # Save the best model based on validation loss\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model = model\n",
        "            save_model(best_model, save_path)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1:03d}/{epochs:03d} - \"\n",
        "              f\"Training Loss: {average_loss:.4f}, \"\n",
        "              f\"Validation Loss: {val_loss:.4f}, \"\n",
        "              f\"Validation Accuracy: {val_accuracy:.4f}, \"\n",
        "              f\"Cohen's Kappa: {kappa:.4f}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# TESTING FUNCTION\n",
        "def test_model(model, data_directory, model_directory, file_name, device,\n",
        "               loss_function):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on the test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained PyTorch model\n",
        "    - data_directory: Directory containing the Test set (Mono or Multi)\n",
        "    - model_directory: Directory to save the test report\n",
        "    - file_name: Name of the report file\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    - loss_function: Loss function used for evaluation\n",
        "\n",
        "    Outputs:\n",
        "    - Writes test report including Loss, Accuracy, Expected Accuracy, Cohen's Kappa\n",
        "    \"\"\"\n",
        "    test_files = glob.glob(os.path.join(data_directory, 'Test/*.pkl'))\n",
        "    X_test, Y_test = data_loader(test_files)\n",
        "\n",
        "    expected_accuracy = compute_expected_accuracy(Y_test)\n",
        "    X_test, Y_test = to_device(X_test, Y_test, device)\n",
        "\n",
        "    test_loss, test_accuracy = validate(model, X_test, Y_test, loss_function)\n",
        "    kappa = compute_kappa(test_accuracy, expected_accuracy)\n",
        "\n",
        "    report = f\"Test Loss: {test_loss:.4f}, \" \\\n",
        "             f\"Test Accuracy: {test_accuracy:.4f}, \" \\\n",
        "             f\"Test Expected Accuracy: {expected_accuracy:.4f}, \" \\\n",
        "             f\"Cohen's Kappa: {kappa:.4f}\"\n",
        "    print(report)\n",
        "\n",
        "    report_file = os.path.join(model_directory, file_name)\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    configurations = load_configurations()\n",
        "    directories = parse_directories()\n",
        "\n",
        "    # Directories for embeddings and model saving\n",
        "    data_directory = os.path.join(\n",
        "        '/content/drive/MyDrive/NeuroScape/data/internal/intermediate/embeddings/classifier')\n",
        "    model_directory = os.path.join(\n",
        "        BASEPATH, directories['internal']['intermediate']['models'])\n",
        "\n",
        "    device = configurations['model']['device']\n",
        "    layer_sizes = configurations['model']['layer_sizes']\n",
        "    num_classes = configurations['model']['num_classes']\n",
        "\n",
        "    # Initialize the classifier\n",
        "    model = DisciplineClassifier(layer_sizes, num_classes).to(device)\n",
        "    loss_function = nn.BCELoss()\n",
        "\n",
        "    pretrain_configurations = configurations['pretraining']\n",
        "    train_configurations = configurations['training']\n",
        "    tune_configurations = configurations['finetuning']\n",
        "\n",
        "    learning_rate = pretrain_configurations['learning_rate']\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define Mono (single-label) and Multi (multi-label) directories\n",
        "    mono_directory = os.path.join(data_directory, 'Monolabel')   # Monolabel = single-discipline articles\n",
        "    multi_directory = os.path.join(data_directory, 'Multilabel') # Multilabel = multi-discipline articles\n",
        "\n",
        "    # PRETRAINING ON MONOLABEL\n",
        "    print(\"Pretraining the model...\")\n",
        "    model = train_model(model, None, mono_directory, model_directory,\n",
        "                        'discipline_classification_model_pretrained',\n",
        "                        pretrain_configurations, device, loss_function,\n",
        "                        optimizer)\n",
        "    print(\"Pretraining completed.\")\n",
        "    print('---' * 10)\n",
        "\n",
        "    # TRAINING ON MULTILABEL\n",
        "    filter_model = deepcopy(model)  # Copy pretrained model for filtering\n",
        "    filter_model.eval()\n",
        "    learning_rate = train_configurations['learning_rate']\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Training the model...\")\n",
        "    model = train_model(model, filter_model, multi_directory, model_directory,\n",
        "                        'discipline_classification_model_trained',\n",
        "                        train_configurations, device, loss_function, optimizer)\n",
        "    print(\"Training completed.\")\n",
        "    print('---' * 10)\n",
        "\n",
        "    # FINETUNING ON MONOLABEL\n",
        "    learning_rate = tune_configurations['learning_rate']\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Finetuning the model...\")\n",
        "    model = train_model(model, None, mono_directory, model_directory,\n",
        "                        'discipline_classification_model_finetuned',\n",
        "                        tune_configurations, device, loss_function, optimizer)\n",
        "    print(\"Finetuning completed.\")\n",
        "    print('---' * 10)\n",
        "\n",
        "    # TESTING PHASE\n",
        "    test_model(model, multi_directory, model_directory,\n",
        "               'multi_label_report.txt', device, loss_function)\n",
        "    test_model(model, mono_directory, model_directory,\n",
        "               'mono_label_report.txt', device, loss_function)\n"
      ],
      "metadata": {
        "id": "qHOPyHY2Ek0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter Data**"
      ],
      "metadata": {
        "id": "c0veYcAtRIWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.utils.filtering import *\n",
        "from src.utils.parsing import parse_directories\n",
        "from src.utils.classifier import get_disciplines\n",
        "from src.utils.load_and_save import save_articles_to_hdf5\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/NeuroScape/')\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv(\"/content/drive/MyDrive/NeuroScape/keys.env\")\n",
        "BASEPATH = os.environ['BASEPATH']\n",
        "\n",
        "def filter(embedding_files, dataframe, device, model, class_index, cutoff):\n",
        "    \"\"\"\n",
        "    Filters articles based on predictions of a neural classification model.\n",
        "\n",
        "    For each embedding file:\n",
        "    - Load article embeddings and their PubMed IDs\n",
        "    - Move embeddings to the target device (CPU/GPU)\n",
        "    - Compute predicted probabilities for each class\n",
        "    - Calculate confidence for the target class as the ratio of its predicted\n",
        "      probability to the maximum predicted probability\n",
        "    - Retrieve the disciplines corresponding to each PubMed ID\n",
        "    - Identify articles to remove based on confidence threshold and custom rules\n",
        "    - Exclude embeddings containing NaN or Inf values\n",
        "    - Update a global list of kept articles and store filtered articles\n",
        "    - Finally, drop removed articles from the dataframe\n",
        "    \"\"\"\n",
        "    global_keep_index = []\n",
        "    filtered_data = []\n",
        "\n",
        "    for file in tqdm(embedding_files, total=len(embedding_files)):\n",
        "        with open(file, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        pubmed_ids = data.pmids\n",
        "        embeddings = data.embeddings\n",
        "\n",
        "        # Move embeddings to device for model inference\n",
        "        embeddings_on_device = torch.tensor(embeddings).to(device)\n",
        "        probabilities = model(embeddings_on_device).cpu().detach().numpy()\n",
        "\n",
        "        # Compute class-specific confidence scores\n",
        "        class_probabilities = probabilities[:, class_index]\n",
        "        max_probabilities = np.max(probabilities, axis=1)\n",
        "        confidence = class_probabilities / max_probabilities\n",
        "\n",
        "        # Get article disciplines and decide which to remove based on confidence\n",
        "        disciplines = get_disciplines(dataframe, pubmed_ids)\n",
        "        remove_indices = remove(disciplines, confidence, cutoff)\n",
        "\n",
        "        # Check for invalid embeddings containing NaN or Inf\n",
        "        nan_indices = np.isnan(embeddings).any(axis=1)\n",
        "        inf_indices = np.isinf(embeddings).any(axis=1)\n",
        "\n",
        "        # Keep only valid articles above confidence threshold\n",
        "        keep_index = [\n",
        "            pubmed_ids[i] for i in range(len(pubmed_ids))\n",
        "            if not remove_indices[i] and not nan_indices[i] and not inf_indices[i]\n",
        "        ]\n",
        "\n",
        "        # Update global list of kept indices\n",
        "        global_keep_index = update_keep_index(global_keep_index, keep_index, dataframe)\n",
        "\n",
        "        # Convert kept articles to structured objects for later processing\n",
        "        for pmid in keep_index:\n",
        "            index = pubmed_ids.index(pmid)\n",
        "            article = fill_article(pmid, dataframe, embeddings[index])\n",
        "            filtered_data.append(article)\n",
        "\n",
        "    # Drop rows from dataframe corresponding to removed articles\n",
        "    drop_index = list(set(dataframe.index) - set(global_keep_index))\n",
        "    dataframe = dataframe.drop(drop_index)\n",
        "\n",
        "    return filtered_data, dataframe\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    configurations = load_configurations()\n",
        "    items_per_shard = configurations['filtering']['shard_size']\n",
        "    neuro_class_index = configurations['filtering']['class_index']\n",
        "    confidence_cutoff = configurations['filtering']['confidence_cutoff']\n",
        "\n",
        "    # Parse directories for data and embeddings\n",
        "    data_directories = parse_directories()\n",
        "\n",
        "    # Load the pre-trained discipline classification model\n",
        "    model_file = os.path.join(\n",
        "        '/content/drive/MyDrive/NeuroScape/data/internal/intermediate/models',\n",
        "        'discipline_classification_model_finetuned.pth'\n",
        "    )\n",
        "    model = load_model(configurations['model'], model_file)\n",
        "    device = configurations['model']['device']\n",
        "\n",
        "    # Set up directories for multidisciplinary and neuroscience data\n",
        "    multidisciplinary_dataframe_dir = os.path.join(\n",
        "        '/content/drive/MyDrive/NeuroScape/output/tratados', 'otherdisciplines'\n",
        "    )\n",
        "    neuroscience_dataframe_dir = os.path.join(\n",
        "        '/content/drive/MyDrive/NeuroScape/output/tratados', 'neuroscience'\n",
        "    )\n",
        "    multidisciplinary_embedding_dir = os.path.join(\n",
        "        '/content/drive/MyDrive/NeuroScape/output/embeddings', 'otherdisciplines'\n",
        "    )\n",
        "    neuroscience_embedding_dir = os.path.join(\n",
        "        '/content/drive/MyDrive/NeuroScape/output/embeddings', 'neuroscience'\n",
        "    )\n",
        "\n",
        "    filtered_data = []\n",
        "\n",
        "    # Load dataframes with article metadata\n",
        "    multi_dataframe = pd.read_csv(\n",
        "        os.path.join(multidisciplinary_dataframe_dir, 'articles_merged_cleaned.csv')\n",
        "    )\n",
        "    neuro_dataframe = pd.read_csv(\n",
        "        os.path.join(neuroscience_dataframe_dir, 'articles_merged_cleaned.csv')\n",
        "    )\n",
        "\n",
        "    # Gather embedding files to process\n",
        "    multi_embedding_files = glob.glob(os.path.join(multidisciplinary_embedding_dir, '*.pkl'))\n",
        "    neuro_embedding_files = glob.glob(os.path.join(neuroscience_embedding_dir, '*.pkl'))\n",
        "\n",
        "    # Filter multidisciplinary articles based on classification confidence\n",
        "    print('Filtering multidisciplinary data...')\n",
        "    multi_filtered_data, multi_dataframe = filter(\n",
        "        multi_embedding_files, multi_dataframe, device, model, neuro_class_index, confidence_cutoff\n",
        "    )\n",
        "\n",
        "    # Filter neuroscience articles\n",
        "    print('Filtering neuroscience data...')\n",
        "    neuro_filtered_data, neuro_dataframe = filter(\n",
        "        neuro_embedding_files, neuro_dataframe, device, model, neuro_class_index, confidence_cutoff\n",
        "    )\n",
        "\n",
        "    # Merge the filtered dataframes into a single dataframe\n",
        "    print('Merging data...')\n",
        "    dataframe = pd.concat([multi_dataframe, neuro_dataframe], ignore_index=True)\n",
        "    filtered_data.extend(multi_filtered_data)\n",
        "    filtered_data.extend(neuro_filtered_data)\n",
        "\n",
        "    # Prepare output directory\n",
        "    output_directory = os.path.join('/content/drive/MyDrive/NeuroScape/output/filtrados')\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Save the filtered dataframe as CSV\n",
        "    df_output_file = os.path.join(neuroscience_dataframe_dir, 'articles_merged_cleaned_filtered.csv')\n",
        "    emb_output_file = os.path.join(output_directory, 'articles_merged_cleaned_filtered.h5')\n",
        "    print('Saving data...')\n",
        "    dataframe.to_csv(df_output_file, index=False)\n",
        "\n",
        "    # Save filtered articles as HDF5 shards for further processing\n",
        "    print('Saving articles...')\n",
        "    for i, start in tqdm(enumerate(range(0, len(filtered_data), items_per_shard))):\n",
        "        file_name = os.path.join(output_directory, f'shard_{i:04d}.h5')\n",
        "        end = start + items_per_shard\n",
        "        save_articles_to_hdf5(filtered_data[start:end], file_name, disable_tqdm=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Q7dKrMEHRJ1M",
        "outputId": "e82fc110-177b-4d27-ee5a-9ca707a0406c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-454216974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         'discipline_classification_model_finetuned.pth')\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigurations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Trabalhos/TJ/NeuroScape/src/utils/filtering.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(configurations, model_file)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisciplineClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m                         \u001b[0;34mf\"Only persistent_load of storage is allowed, but got {pid[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                     )\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2081\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   2084\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_fake_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m             \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fake_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \"\"\"\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mbackend_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_privateuse1_backend_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mdevice_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"is_available\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdevice_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0;34mf\"Attempting to deserialize object on a {backend_name.upper()} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;34mf\"device but torch.{backend_name}.is_available() is False. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    }
  ]
}